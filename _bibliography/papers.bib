---
---

@article{min2023workflow,
  title={Workflow-Guided Response Generation for Task-Oriented Dialogue},
  author={Min, Do June and Sodhi, Paloma and Ramakrishnan, Ramya},
  journal={arXiv preprint arXiv:2311.08300},
  year={2023},
  pdf={workflow_response_generation.pdf},
  abstract={Task-oriented dialogue (TOD) systems aim to achieve specific goals through interactive dialogue. Such tasks usually involve following specific workflows, i.e. executing a sequence of actions in a particular order. While prior work has focused on supervised learning methods to condition on past actions, they do not explicitly optimize for compliance to a desired workflow. In this paper, we propose a novel framework based on reinforcement learning (RL) to generate dialogue responses that are aligned with a given workflow. Our framework consists of ComplianceScorer, a metric designed to evaluate how well a generated response executes the specified action, combined with an RL opimization process that utilizes an interactive sampling technique. We evaluate our approach on two TOD datasets, Action-Based Conversations Dataset (ABCD) (Chen et al., 2021a) and MultiWOZ 2.2 (Zang et al., 2020) on a range of automated and human evaluation metrics. Our findings indicate that our RL-based framework outperforms baselines and is effective at enerating responses that both comply with the intended workflows while being expressed in a natural and fluent manner.},
  bibtex_show={true},
  selected={true},
}

@article{ramakrishnan2023multi,
  title={Multi-Step Dialogue Workflow Action Prediction},
  author={Ramakrishnan, Ramya and Elenberg, Ethan and Narangodage, Hashan and McDonald, Ryan},
  journal={arXiv preprint arXiv:2311.09593},
  year={2023},
  pdf={multi_step_action_prediction.pdf},
  abstract={In task-oriented dialogue, a system often needs to follow a sequence of actions, called a workflow, that complies with a set of guidelines in order to complete a task. In this paper, we propose the novel problem of multi-step workflow action prediction, in which the system predicts multiple future workflow actions. Accurate prediction of multiple steps allows for multi-turn automation, which can free up time to focus on more complex tasks. We propose three modeling approaches that are simple to implement yet lead to more action automation: 1) fine-tuning on a training dataset, 2) few-shot in-context learning leveraging retrieval and large language model prompting, and 3) zero-shot graph traversal, which aggregates historical action sequences into a graph for prediction. We show that multi-step action prediction produces features that improve accuracy on downstream dialogue tasks like predicting task success, and can increase automation of steps by 20% without requiring as much feedback from a human overseeing the system.},
  bibtex_show={true},
  selected={true}
}

@article{ramakrishnan2022long,
  title={Long-term control for dialogue generation: Methods and evaluation},
  author={Ramakrishnan, Ramya and Narangodage, Hashan Buddhika and Schilman, Mauro and Weinberger, Kilian Q and McDonald, Ryan},
  journal={arXiv preprint arXiv:2205.07352},
  year={2022},
  bibtex_show={true},
  pdf={naacl_2022_long_term_control.pdf},
  abstract={Current approaches for controlling dialogue response generation are primarily focused on high-level attributes like style, sentiment, or topic. In this work, we focus on constrained long-term dialogue generation, which involves more fine-grained control and requires a given set of control words to appear in generated responses. This setting requires a model to not only consider the generation of these control words in the immediate context, but also produce utterances that will encourage the generation of the words at some time in the (possibly distant) future. We define the problem of constrained long-term control for dialogue generation, identify gaps in current methods for evaluation, and propose new metrics that better measure long-term control. We also propose a retrieval-augmented method that improves performance of long-term controlled generation via logit modification techniques. We show through experiments on three task-oriented dialogue datasets that our metrics better assess dialogue control relative to current alternatives and that our method outperforms state-of-the-art constrained generation baselines.},
  bibtex_show={true},
  selected={true}
}

@article{ramakrishnan2021bayesian,
  title={A bayesian approach to identifying representational errors},
  author={Ramakrishnan, Ramya and Unhelkar, Vaibhav and Kamar, Ece and Shah, Julie},
  journal={arXiv preprint arXiv:2103.15171},
  year={2021},
  pdf={bayesian_representation_errors.pdf},
  abstract={Trained AI systems and expert decision makers can make errors that are often difficult to identify and understand. Determining the root cause for these errors can improve future decisions. This work presents Generative Error Model (GEM), a generative model for inferring representational errors based on observations of an actor's behavior (either simulated agent, robot, or human). The model considers two sources of error: those that occur due to representational limitations -- "blind spots" -- and non-representational errors, such as those caused by noise in execution or systematic errors present in the actor's policy. Disambiguating these two error types allows for targeted refinement of the actor's policy (i.e., representational errors require perceptual augmentation, while other errors can be reduced through methods such as improved training or attention support). We present a Bayesian inference algorithm for GEM and evaluate its utility in recovering representational errors on multiple domains. Results show that our approach can recover blind spots of both reinforcement learning agents as well as human users.},
  bibtex_show={true}
}

@article{ramakrishnan2020blind,
  title={Blind spot detection for safe sim-to-real transfer},
  author={Ramakrishnan, Ramya and Kamar, Ece and Dey, Debadeepta and Horvitz, Eric and Shah, Julie},
  journal={Journal of Artificial Intelligence Research},
  volume={67},
  pages={191--234},
  year={2020},
  pdf={jair_2020_blind_spot_detection.pdf},
  abstract={Agents trained in simulation may make errors when performing actions in the real world due to mismatches between training and execution environments. These mistakes can be dangerous and difficult for the agent to discover because the agent is unable to predict them a priori. In this work, we propose the use of oracle feedback to learn a predictive model of these blind spots in order to reduce costly errors in real-world applications. We focus on blind spots in reinforcement learning (RL) that occur due to incomplete state representation: when the agent lacks necessary features to represent the true state of the world, and thus cannot distinguish between numerous states. We formalize the problem of discovering blind spots in RL as a noisy supervised learning problem with class imbalance. Our system learns models for predicting blind spots within unseen regions of the state space by combining techniques for label aggregation, calibration, and supervised learning. These models take into consideration noise emerging from different forms of oracle feedback, including demonstrations and corrections. We evaluate our approach across two domains and demonstrate that it achieves higher predictive performance than baseline methods, and also that the learned model can be used to selectively query an oracle at execution time to prevent errors. We also empirically analyze the biases of various feedback types and how these biases influence the discovery of blind spots. Further, we include analyses of our approach that incorporate relaxed initial optimality assumptions. (Interestingly, relaxing the assumptions of an optimal oracle and an optimal simulator policy helped our models to perform better.) We also propose extensions to our method that are intended to improve performance when using corrections and demonstrations data.},
  bibtex_show={true}
}

@inproceedings{ramakrishnan2019overcoming,
  title={Overcoming blind spots in the real world: Leveraging complementary abilities for joint execution},
  author={Ramakrishnan, Ramya and Kamar, Ece and Nushi, Besmira and Dey, Debadeepta and Shah, Julie and Horvitz, Eric},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={33},
  number={01},
  pages={6137--6145},
  year={2019},
  pdf={aaai_2019_overcoming_blind_spots.pdf},
  abstract={Simulators are being increasingly used to train agents before deploying them in real-world environments. While training in simulation provides a cost-effective way to learn, poorly modeled aspects of the simulator can lead to costly mistakes, or blind spots. While humans can help guide an agent towards identifying these error regions, humans themselves have blind spots and noise in execution. We study how learning about blind spots of both can be used to manage hand-off decisions when humans and agents jointly act in the real-world in which neither of them are trained or evaluated fully. The formulation assumes that agent blind spots result from representational limitations in the simulation world, which leads the agent to ignore important features that are relevant for acting in the open world. Our approach for blind spot discovery combines experiences collected in simulation with limited human demonstrations. The first step applies imitation learning to demonstration data to identify important features that the human is using but that the agent is missing. The second step uses noisy labels extracted from action mismatches between the agent and the human across simulation and demonstration data to train blind spot models. We show through experiments on two domains that our approach is able to learn a succinct representation that accurately captures blind spot regions and avoids dangerous errors in the real world through transfer of control between the agent and the human.},
  bibtex_show={true}
}

@article{ramakrishnan2018discovering,
  title={Discovering blind spots in reinforcement learning},
  author={Ramakrishnan, Ramya and Kamar, Ece and Dey, Debadeepta and Shah, Julie and Horvitz, Eric},
  journal={arXiv preprint arXiv:1805.08966},
  year={2018},
  pdf={aamas_2018_discovering_blind_spots.pdf},
  abstract={Agents trained in simulation may make errors in the real world due to mismatches between training and execution environments. These mistakes can be dangerous and difficult to discover because the agent cannot predict them a priori. We propose using oracle feedback to learn a predictive model of these blind spots to reduce costly errors in real-world applications. We focus on blind spots in reinforcement learning (RL) that occur due to incomplete state representation: The agent does not have the appropriate features to represent the true state of the world and thus cannot distinguish among numerous states. We formalize the problem of discovering blind spots in RL as a noisy supervised learning problem with class imbalance. We learn models to predict blind spots in unseen regions of the state space by combining techniques for label aggregation, calibration, and supervised learning. The models take into consideration noise emerging from different forms of oracle feedback, including demonstrations and corrections. We evaluate our approach on two domains and show that it achieves higher predictive performance than baseline methods, and that the learned model can be used to selectively query an oracle at execution time to prevent errors. We also empirically analyze the biases of various feedback types and how they influence the discovery of blind spots.},
  bibtex_show={true}
}

@article{ramakrishnan2017perturbation,
  title={Perturbation training for human-robot teams},
  author={Ramakrishnan, Ramya and Zhang, Chongjie and Shah, Julie},
  journal={Journal of Artificial Intelligence Research},
  volume={59},
  pages={495--541},
  year={2017},
  pdf={jair_2017_perturbation_training.pdf},
  abstract={In this work, we design and evaluate a computational learning model that enables a human-robot team to co-develop joint strategies for performing novel tasks that require coordination. The joint strategies are learned through "perturbation training," a human team-training strategy that requires team members to practice variations of a given task to help their team generalize to new variants of that task. We formally define the problem of human-robot perturbation training and develop and evaluate the first end-to-end framework for such training, which incorporates a multi-agent transfer learning algorithm, human-robot co-learning framework and communication protocol. Our transfer learning algorithm, Adaptive Perturbation Training (AdaPT), is a hybrid of transfer and reinforcement learning techniques that learns quickly and robustly for new task variants. We empirically validate the benefits of AdaPT through comparison to other hybrid reinforcement and transfer learning techniques aimed at transferring knowledge from multiple source tasks to a single target task. We also demonstrate that AdaPT's rapid learning supports live interaction between a person and a robot, during which the human-robot team trains to achieve a high level of performance for new task variants. We augment AdaPT with a co-learning framework and a computational bi-directional communication protocol so that the robot can co-train with a person during live interaction. Results from large-scale human subject experiments (n=48) indicate that AdaPT enables an agent to learn in a manner compatible with a human's own learning process, and that a robot undergoing perturbation training with a human results in a high level of team performance. Finally, we demonstrate that human-robot training using AdaPT in a simulation environment produces effective performance for a team incorporating an embodied robot partner.},
  bibtex_show={true}
}

@inproceedings{nikolaidis2015efficient,
  title={Efficient model learning from joint-action demonstrations for human-robot collaborative tasks},
  author={Nikolaidis, Stefanos and Ramakrishnan, Ramya and Gu, Keren and Shah, Julie},
  booktitle={Proceedings of the tenth annual ACM/IEEE international conference on human-robot interaction},
  pages={189--196},
  year={2015},
  pdf={hri_2015_efficient_model_learning.pdf},
  abstract={We present a framework for automatically learning human user models from joint-action demonstrations that enables a robot to compute a robust policy for a collaborative task with a human. First, the demonstrated action sequences are clustered into different human types using an unsupervised learning algorithm. A reward function is then learned for each type through the employment of an inverse reinforcement learning algorithm. The learned model is then incorporated into a mixed-observability Markov decision process (MOMDP) formulation, wherein the human type is a partially observable variable. With this framework, we can infer online the human type of a new user that was not included in the training set, and can compute a policy for the robot that will be aligned to the preference of this user. In a human subject experiment (n=30), participants agreed more strongly that the robot anticipated their actions when working with a robot incorporating the proposed framework (p<0.01), compared to manually annotating robot actions. In trials where participants faced difficulty annotating the robot actions to complete the task, the proposed framework significantly improved team efficiency (p<0.01). The robot incorporating the framework was also found to be more responsive to human actions compared to policies computed using a hand-coded reward function by a domain expert (p<0.01). These results indicate that learning human user models from joint-action demonstrations and encoding them in a MOMDP formalism can support effective teaming in human-robot collaborative tasks.},
  bibtex_show={true}
}

@article{nikolaidis2015improved,
  title={Improved human--robot team performance through cross-training, an approach inspired by human team training practices},
  author={Nikolaidis, Stefanos and Lasota, Przemyslaw and Ramakrishnan, Ramya and Shah, Julie},
  journal={The International Journal of Robotics Research},
  volume={34},
  number={14},
  pages={1711--1730},
  year={2015},
  publisher={SAGE Publications Sage UK: London, England},
  pdf={ijrr_2015_improved_human_robot.pdf},
  abstract={We design and evaluate a method of human–robot cross-training, a validated and widely used strategy for the effective training of human teams. Cross-training is an interactive planning method in which team members iteratively switch roles with one another to learn a shared plan for the performance of a collaborative task. We first present a computational formulation of the robot mental model, which encodes the sequence of robot actions necessary for task completion and the expectations of the robot for preferred human actions, and show that the robot model is quantitatively comparable to the mental model that captures the inter-role knowledge held by the human. Additionally, we propose a quantitative measure of robot mental model convergence and an objective metric of model similarity. Based on this encoding, we formulate a human–robot cross-training method and evaluate its efficacy through experiments involving human subjects. We compare human–robot cross-training to standard reinforcement learning techniques, and show that cross-training yields statistically significant improvements in quantitative team performance measures, as well as significant differences in perceived robot performance and human trust. Finally, we discuss the objective measure of robot mental model convergence as a method to dynamically assess human errors. This study supports the hypothesis that the effective and fluent teaming of a human and a robot may best be achieved by modeling known, effective human teamwork practices.},
  bibtex_show={true}
}

@article{ramakrishnan100knowledge,
  title={Knowledge Transfer from a Human Perspective},
  author={Ramakrishnan, Ramya and Shah, Julie A},
  journal={Transfer in Reinforcement Learning Workshop at AAMAS 2017},
  volume={100},
  number={125},
  pages={150},
  pdf={tirl_2017_knowledge_transfer.pdf},
  abstract={Transfer in reinforcement learning (TiRL) is a challenging research problem. Agents are still preprogrammed for specific tasks or can only transfer knowledge under limited circumstances. To quicken the learning process, people, who are extremely adaptable, may be able to provide feedback to guide the agent. This requires an interpretable medium for transfer that both the human and agent can understand. In this work, we propose a transfer medium based on object mappings between tasks. We conduct human subject experiments to test whether people are able to effectively use these mappings in the form of advice to play video games. Preliminary results show that good mappings improve people’s transfer performance on some games, but can hurt people when they misunderstand. The potential interpretability benefits of using an object-based representation for TiRL can guide the development of more interpretable transfer learning algorithms for agents.},
  bibtex_show={true}
}

@inproceedings{ramakrishnan2016interpretable,
  title={Interpretable transfer for reinforcement learning based on object similarities},
  author={Ramakrishnan, Ramya and Narasimhan, Karthik and Shah, Julie},
  booktitle={Proceedings of the IJCAI Interactive Machine Learning Workshop},
  year={2016},
  organization={sn},
  bibtex_show={true}
}

@phdthesis{ramakrishnan2019error,
  title={Error discovery through human-AI collaboration},
  author={Ramakrishnan, Ramya},
  year={2019},
  school={Massachusetts Institute of Technology},
  pdf={mit_phd_thesis.pdf},
  abstract={While there has been a recent rise in increasingly effective human-Al teams in areas such as autonomous driving, manufacturing, and robotics, many catastrophic failures still occur. Understanding the cause(s) of these errors is crucial for reducing and fixing them. One source of error is due to an agent's or human's limited view of the world, which means their representations are insufficient for acting safely. For example, self-driving cars may have limited sensing that causes them to not recognize rare vehicle types, like emergency vehicles. This thesis focuses on identifying errors that occur due to deficiencies in agent and human representations. In the first part, we develop an approach that uses human feedback to identify agent errors that occur due to an agent's limited state representation, meaning that the agent cannot observe all features of the world. Experiments show that using our model, an agent discovers error regions and is able to query for human help intelligently to safely act in the real world. In the second part, we focus on determining the cause of human errors as either occurring due to the human's flawed observation of the world or due to other factors, such as noise or insufficient training. We present a generative model that approximates the human's decision-making process and show that we can infer the latent error sources with a limited amount of human demonstration data. In the final thesis component, we tackle the setting where both an agent and a human have rich perception, but due to selective attention, they each only focus on a subset of features. When deploying these learned policies, important features in the real world may be ignored because the simulator did not accurately model all regions of the real world. Our approach is able to identify scenarios in which an agent should transfer control to a human who may be better suited to act, leading to safe joint execution in the world.},
  bibtex_show={true}
}

@inproceedings{ramakrishnan2016towards,
  title={Towards interpretable explanations for transfer learning in sequential tasks},
  author={Ramakrishnan, Ramya and Shah, Julie},
  booktitle={2016 AAAI Spring Symposium Series},
  year={2016},
  pdf={aaai_2016_towards_explanations.pdf},
  abstract={People increasingly rely on machine learning (ML) to make intelligent decisions. However, the ML results are often difficult to interpret and the algorithms do not support interaction to solicit clarification or explanation. In this paper, we highlight an emerging research area of interpretable explanations for transfer learning in sequential tasks, in which an agent must explain how it learns a new task given prior, common knowledge. The goal is to enhance a user’s ability to trust and use the system output and to enable iterative feedback for improving the system. We review prior work in probabilistic systems, sequential decision-making, interpretable explanations, transfer learning, and interactive machine learning, and identify an intersection that deserves further research focus. We believe that developing adaptive, transparent learning models will build the foundation for better human-machine systems in applications for elder care, education, and health care.},
  bibtex_show={true}
}

@inproceedings{nikolaidis2014learning,
  title={Learning Human Types from Demonstration},
  author={Nikolaidis, Stefanos and Gu, Keren and Ramakrishnan, Ramya and Shah, Julie},
  booktitle={2014 AAAI Fall Symposium Series},
  year={2014},
  pdf={aaai_2014_learning_human_types.pdf},
  abstract={The development of new industrial robotic systems that operate in the same physical space as people highlights the emerging need for robots that can integrate seamlessly into human group dynamics by adapting to the personalized style of human teammates. This adaptation requires learning a statistical model of human behavior and integrating this model into the decision-making algorithm of the robot in a principled way. We present a framework for automatically learning human user models from joint-action demonstrations that enables the robot to compute a robust policy for a collaborative task with a human, assuming access to demonstrations of human teams working on the task. The robustness of the action selection mechanism of the robot is compared to previous model-learning algorithms in the ability to function despite increasing deviations of human actions from previously demonstrated behavior.},
  bibtex_show={true}
}

@inproceedings{nybakke2012virtual,
  title={From virtual to actual mobility: Assessing the benefits of active locomotion through an immersive virtual environment using a motorized wheelchair},
  author={Nybakke, Amelia and Ramakrishnan, Ramya and Interrante, Victoria},
  booktitle={2012 IEEE Symposium on 3D User Interfaces (3DUI)},
  pages={27--30},
  year={2012},
  organization={IEEE},
  pdf={3dui12_virtual_to_actual_mobility.pdf},
  abstract={As we move around, in a real or virtual environment, the process of keeping track of where we are, in relation to the portions of the environment that are out of view, is referred to as spatial updating. Studies have shown that in the real world, when we use real walking to get around, this process is both effortless and automatic, but that in virtual environments, when purely virtual methods of locomotion are used, the accuracy and ease of spatial updating is significantly diminished. In this paper, we present the results of an experiment intended to assess the impact, on spatial updating performance, of enabling people to physically move about in an immersive virtual environment using a motorized wheelchair. This study is motivated by an interest in probing the potential of re-directed driving as an alternative method for enabling people to effectively explore a relatively larger virtual space while physically moving about in a smaller actual space. A total of 24 participants in our within-subjects experiment traveled through a 24' wide circularly symmetric virtual room, searching the contents of 16 randomly positioned and oriented boxes to locate 8 hidden targets, using each of the following four locomotion methods: real walking (R), virtual translation with real rotation by standing and using a body-worn joystick (S), real driving in a motorized wheelchair (W), and virtual translation with real rotation by sitting in a swivel chair and using a joystick mounted on one of its arms (J). We computed four measures of search efficiency: total distance traveled, total number of targets revisited, proportion of perfect trials, and total search time. Overall, we found that performance was significantly better with real walking than with either of the virtual travel methods, consistent with most previous findings, and that performance with the wheelchair was intermediate. These results suggest some advantage in enabling actual, as opposed to purely virtual, translational movement in a locomotion interface, and lend support to the potential viability of a re-directed driving implementation.},
  bibtex_show={true}
}

@phdthesis{ramakrishnan2015perturbation,
  title={Perturbation training for human-robot teams},
  author={Ramakrishnan, Ramya},
  year={2015},
  school={Massachusetts Institute of Technology},
  pdf={mit_masters_thesis.pdf},
  abstract={Today, robots are often deployed to work separately from people. Combining the strengths of humans and robots, however, can potentially lead to a stronger joint team. To have fluid human-robot collaboration, these teams must train to achieve high team performance and flexibility on new tasks. This requires a computational model that supports the human in learning and adapting to new situations. In this work, we design and evaluate a computational learning model that enables a human-robot team to co-develop joint strategies for performing novel tasks requiring coordination. The joint strategies are learned through "perturbation training," a human team-training strategy that requires practicing variations of a given task to help the team generalize to new variants of that task. Our Adaptive Perturbation Training (AdaPT) algorithm is a hybrid of transfer learning and reinforcement learning techniques and extends the Policy Reuse in Q-Learning (PRQL) algorithm to learn more quickly in new task variants. We empirically validate this advantage of AdaPT over PRQL through computational simulations. We then augment our algorithm AdaPT with a co-learning framework and a computational bi-directional communication protocol so that the robot can work with a person in live interactions. These three features constitute our human-robot perturbation training model. We conducted human subject experiments to show proof-of-concept that our model enables a robot to draw from its library of prior experiences in a way that leads to high team performance. We compare our algorithm with a standard reinforcement learning algorithm Q-learning and find that AdaPT-trained teams achieved significantly higher reward on novel test tasks than Q-learning teams. This indicates that the robot's algorithm, rather than just the human's experience of perturbations, is key to achieving high team performance. We also show that our algorithm does not sacrifice performance on the base task after training on perturbations. Finally, we demonstrate that human-robot training in a simulation environment using AdaPT produced effective team performance with an embodied robot partner.},
  bibtex_show={true}
}